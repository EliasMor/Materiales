{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL1BCwnxULXJ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AUTOMATIZACIÓN CON SCRIPTS - NIVEL INTERMEDIO\n",
        "# ============================================================================\n",
        "\n",
        "#crear_banner(\"AUTOMATIZACIÓN CON SCRIPTS DE PYTHON\")\n",
        "\n",
        "print(\" **Objetivo:** Crear scripts que se ejecuten automáticamente\")\n",
        "print(\" **Aprenderemos:**\")\n",
        "print(\"   • Crear funciones reutilizables\")\n",
        "print(\"   • Manejar errores\")\n",
        "print(\"   • Logging (registros)\")\n",
        "print(\"   • Configuración externa\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# SCRIPT 1: PROCESADOR DE ARCHIVOS AUTOMÁTICO\n",
        "# ============================================================================\n",
        "\n",
        "print(\" **SCRIPT 1: Procesador automático de archivos**\")\n",
        "print(\"Este script busca archivos nuevos y los procesa automáticamente\")\n",
        "print()\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('etl_process.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "class ProcesadorETL:\n",
        "    \"\"\"Clase para automatizar procesos ETL\"\"\"\n",
        "\n",
        "    def __init__(self, config_file='config.json'):\n",
        "        \"\"\"Inicializar con configuración\"\"\"\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.config = self.cargar_configuracion(config_file)\n",
        "\n",
        "\n",
        "    def cargar_configuracion(self, config_file):\n",
        "        \"\"\"Cargar configuración desde archivo JSON\"\"\"\n",
        "        config_default = {\n",
        "            \"carpeta_entrada\": \"datos_entrada\",\n",
        "            \"carpeta_salida\": \"datos_procesados\",\n",
        "            \"carpeta_backup\": \"backup\",\n",
        "            \"formatos_permitidos\": [\".csv\", \".xlsx\", \".json\"],\n",
        "            \"separador_csv\": \",\",\n",
        "            \"encoding\": \"utf-8\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(config_file):\n",
        "                with open(config_file, 'r') as f:\n",
        "                    config = json.load(f)\n",
        "                self.logger.info(f\"Configuración cargada desde {config_file}\")\n",
        "            else:\n",
        "                config = config_default\n",
        "                # Crear archivo de configuración\n",
        "                with open(config_file, 'w') as f:\n",
        "                    json.dump(config_default, f, indent=4)\n",
        "                self.logger.info(f\"Archivo de configuración creado: {config_file}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error cargando configuración: {e}\")\n",
        "            config = config_default\n",
        "\n",
        "        return config\n",
        "\n",
        "    def crear_carpetas(self):\n",
        "        \"\"\"Crear carpetas necesarias si no existen\"\"\"\n",
        "        carpetas = [\n",
        "            self.config['carpeta_entrada'],\n",
        "            self.config['carpeta_salida'],\n",
        "            self.config['carpeta_backup']\n",
        "        ]\n",
        "\n",
        "        for carpeta in carpetas:\n",
        "            if not os.path.exists(carpeta):\n",
        "                os.makedirs(carpeta)\n",
        "                self.logger.info(f\"Carpeta creada: {carpeta}\")\n",
        "\n",
        "    def buscar_archivos_nuevos(self):\n",
        "        \"\"\"Buscar archivos para procesar\"\"\"\n",
        "        carpeta = self.config['carpeta_entrada']\n",
        "        formatos = self.config['formatos_permitidos']\n",
        "\n",
        "        archivos = []\n",
        "        if os.path.exists(carpeta):\n",
        "            for archivo in os.listdir(carpeta):\n",
        "                if any(archivo.endswith(fmt) for fmt in formatos):\n",
        "                    ruta_completa = os.path.join(carpeta, archivo)\n",
        "                    archivos.append(ruta_completa)\n",
        "\n",
        "        self.logger.info(f\"Encontrados {len(archivos)} archivos para procesar\")\n",
        "        return archivos\n",
        "\n",
        "    def extraer_datos(self, archivo):\n",
        "        \"\"\"Extraer datos según el tipo de archivo\"\"\"\n",
        "        try:\n",
        "            extension = os.path.splitext(archivo)[1].lower()\n",
        "\n",
        "            if extension == '.csv':\n",
        "                df = pd.read_csv(archivo,\n",
        "                               sep=self.config['separador_csv'],\n",
        "                               encoding=self.config['encoding'])\n",
        "            elif extension == '.xlsx':\n",
        "                df = pd.read_excel(archivo)\n",
        "            elif extension == '.json':\n",
        "                df = pd.read_json(archivo)\n",
        "            else:\n",
        "                raise ValueError(f\"Formato no soportado: {extension}\")\n",
        "\n",
        "            self.logger.info(f\"Datos extraídos de {archivo}: {len(df)} filas\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extrayendo datos de {archivo}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def transformar_datos(self, df, nombre_archivo):\n",
        "        \"\"\"Aplicar transformaciones estándar\"\"\"\n",
        "        try:\n",
        "            df_transformado = df.copy()\n",
        "\n",
        "            # 1. Limpiar espacios en columnas de texto\n",
        "            for col in df_transformado.select_dtypes(include=['object']).columns:\n",
        "                df_transformado[col] = df_transformado[col].astype(str).str.strip()\n",
        "\n",
        "            # 2. Convertir fechas si existen columnas con 'fecha' en el nombre\n",
        "            fecha_cols = [col for col in df_transformado.columns if 'fecha' in col.lower()]\n",
        "            for col in fecha_cols:\n",
        "                df_transformado[col] = pd.to_datetime(df_transformado[col], errors='coerce')\n",
        "\n",
        "            # 3. Agregar metadatos\n",
        "            df_transformado['archivo_origen'] = nombre_archivo\n",
        "            df_transformado['fecha_procesamiento'] = datetime.now()\n",
        "\n",
        "            # 4. Eliminar duplicados\n",
        "            filas_antes = len(df_transformado)\n",
        "            df_transformado = df_transformado.drop_duplicates()\n",
        "            filas_despues = len(df_transformado)\n",
        "\n",
        "            if filas_antes != filas_despues:\n",
        "                self.logger.info(f\"Eliminados {filas_antes - filas_despues} duplicados\")\n",
        "\n",
        "            self.logger.info(f\"Datos transformados: {len(df_transformado)} filas finales\")\n",
        "            return df_transformado\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error transformando datos: {e}\")\n",
        "            return None\n",
        "\n",
        "    def cargar_datos(self, df, nombre_archivo):\n",
        "        \"\"\"Guardar datos procesados\"\"\"\n",
        "        try:\n",
        "            # Crear nombre de archivo de salida\n",
        "            nombre_base = os.path.splitext(os.path.basename(nombre_archivo))[0]\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            nombre_salida = f\"{nombre_base}_procesado_{timestamp}.csv\"\n",
        "\n",
        "            ruta_salida = os.path.join(self.config['carpeta_salida'], nombre_salida)\n",
        "\n",
        "            # Guardar datos\n",
        "            df.to_csv(ruta_salida, index=False, encoding=self.config['encoding'])\n",
        "\n",
        "            self.logger.info(f\"Datos guardados en: {ruta_salida}\")\n",
        "            return ruta_salida\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error guardando datos: {e}\")\n",
        "            return None\n",
        "\n",
        "    def mover_a_backup(self, archivo):\n",
        "        \"\"\"Mover archivo procesado a backup\"\"\"\n",
        "        try:\n",
        "            nombre_archivo = os.path.basename(archivo)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            nombre_backup = f\"{timestamp}_{nombre_archivo}\"\n",
        "\n",
        "            ruta_backup = os.path.join(self.config['carpeta_backup'], nombre_backup)\n",
        "\n",
        "            # Mover archivo\n",
        "            os.rename(archivo, ruta_backup)\n",
        "            self.logger.info(f\"Archivo movido a backup: {ruta_backup}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error moviendo archivo a backup: {e}\")\n",
        "\n",
        "    def procesar_archivo(self, archivo):\n",
        "        \"\"\"Procesar un archivo completo (ETL)\"\"\"\n",
        "        self.logger.info(f\"Iniciando procesamiento de: {archivo}\")\n",
        "\n",
        "        # EXTRACT\n",
        "        df = self.extraer_datos(archivo)\n",
        "        if df is None:\n",
        "            return False\n",
        "\n",
        "        # TRANSFORM\n",
        "        df_transformado = self.transformar_datos(df, archivo)\n",
        "        if df_transformado is None:\n",
        "            return False\n",
        "\n",
        "        # LOAD\n",
        "        archivo_salida = self.cargar_datos(df_transformado, archivo)\n",
        "        if archivo_salida is None:\n",
        "            return False\n",
        "\n",
        "        # Mover a backup\n",
        "        self.mover_a_backup(archivo)\n",
        "\n",
        "        self.logger.info(f\"Procesamiento completado exitosamente: {archivo}\")\n",
        "        return True\n",
        "\n",
        "    def ejecutar_proceso_completo(self):\n",
        "        \"\"\"Ejecutar proceso ETL completo\"\"\"\n",
        "        self.logger.info(\"=== INICIANDO PROCESO ETL AUTOMÁTICO ===\")\n",
        "\n",
        "        # Crear carpetas necesarias\n",
        "        self.crear_carpetas()\n",
        "\n",
        "        # Buscar archivos\n",
        "        archivos = self.buscar_archivos_nuevos()\n",
        "\n",
        "        if not archivos:\n",
        "            self.logger.info(\"No hay archivos para procesar\")\n",
        "            return\n",
        "\n",
        "        # Procesar cada archivo\n",
        "        exitosos = 0\n",
        "        fallidos = 0\n",
        "\n",
        "        for archivo in archivos:\n",
        "            if self.procesar_archivo(archivo):\n",
        "                exitosos += 1\n",
        "            else:\n",
        "                fallidos += 1\n",
        "\n",
        "        self.logger.info(f\"=== PROCESO COMPLETADO ===\")\n",
        "        self.logger.info(f\"Archivos procesados exitosamente: {exitosos}\")\n",
        "        self.logger.info(f\"Archivos con errores: {fallidos}\")\n",
        "\n",
        "# Demostración del procesador\n",
        "print(\" **Creando y ejecutando el procesador ETL automático:**\")\n",
        "\n",
        "# Crear datos de ejemplo para demostrar\n",
        "os.makedirs('datos_entrada', exist_ok=True)\n",
        "\n",
        "# Crear archivo CSV de ejemplo\n",
        "datos_ejemplo = pd.DataFrame({\n",
        "    'fecha': ['2024-01-15', '2024-01-16', '2024-01-17'],\n",
        "    'producto': ['  Laptop  ', 'MOUSE', 'teclado  '],\n",
        "    'precio': [1200.50, 25.99, 45.0],\n",
        "    'cantidad': [1, 2, 1],\n",
        "    'vendedor': ['Ana García', 'Luis Martín', 'María López']\n",
        "})\n",
        "\n",
        "datos_ejemplo.to_csv('datos_entrada/ventas_ejemplo.csv', index=False)\n",
        "print(\" Archivo de ejemplo creado: datos_entrada/ventas_ejemplo.csv\")\n",
        "\n",
        "# Ejecutar el procesador\n",
        "procesador = ProcesadorETL()\n",
        "procesador.ejecutar_proceso_completo()\n",
        "\n",
        "print(\"\\n **Archivos generados:**\")\n",
        "print(\"    config.json - Configuración del procesador\")\n",
        "print(\"    etl_process.log - Log de ejecución\")\n",
        "print(\"    datos_procesados/ - Archivos procesados\")\n",
        "print(\"    backup/ - Archivos originales respaldados\")"
      ]
    }
  ]
}